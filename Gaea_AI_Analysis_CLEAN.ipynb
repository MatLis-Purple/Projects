{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b7d8d6-d0c1-4a8b-b933-6b39716dad4a",
   "metadata": {},
   "source": [
    "# **DA401 - Team Echo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba584b3-26df-4023-8d55-5a7c01313b23",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "During the 2022 heatwave, Spain experienced an unprecedented national shortage of ice, disrupting sales and exposing inefficiencies in the supply chain. Demand for ice products, a staple during high-temperature periods, surged unpredictably, leading to stock shortages and lost revenue. For instance, one store sold out of ice within an hour of opening, while another exceeded annual sales over a single weekend.\n",
    "\n",
    "This project analyses historical sales and climate data from 2021 and 2022 for petrol stations within a 65km radius of Madrid. \n",
    "\n",
    "The objective is to identify key factors influencing ice sales, such as temperature thresholds, seasonal trends, and consumer behaviour during extreme weather events.\n",
    "\n",
    "### Business Problem\n",
    "The retailer struggles to align supply with fluctuating demand for ice products, particularly during extreme weather, resulting in stock shortages, missed sales opportunities, and inefficiencies in inventory management. Understanding the factors influencing demand—such as environmental conditions, seasonality, and location—can optimise inventory, improve profitability, and minimise revenue loss during peak periods.\n",
    "\n",
    "Key questions will include:\n",
    "- How do extreme weather conditions affect consumer demand for ice compared to other items?\n",
    "- How do sales trends fluctuate during festivals, holidays, and seasonal changes?es?\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2127d28-79f5-46c0-adc3-7df9c0e095e6",
   "metadata": {},
   "source": [
    "## **A. Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a241095-6738-455b-90e8-23c18872c194",
   "metadata": {},
   "source": [
    "### **1. Load and explore the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d238b-bc7f-4a00-8f69-0a6b230dbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a8605-f0fa-4219-a299-d854c0a63689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install geopy here - Uncomment and run the next line\n",
    "\n",
    "#pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e874d4-3496-4550-9084-75a2f7601a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Sales data - there are 2 sheets in the excel file.\n",
    "\n",
    "# Load both sheets from the Excel file\n",
    "raw_sales_data = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Notebook\\2021 2022 Client Sales.xlsx\"\n",
    "sheet_names = pd.ExcelFile(raw_sales_data).sheet_names\n",
    "print(f\"Available sheets: {sheet_names}\")\n",
    "\n",
    "# Load the specific sheets into separate DataFrames\n",
    "sales_data_sheet1 = pd.read_excel(raw_sales_data, sheet_name=sheet_names[0])\n",
    "sales_data_sheet2 = pd.read_excel(raw_sales_data, sheet_name=sheet_names[1])\n",
    "\n",
    "# Display the first few rows of each sheet\n",
    "print(\"Sales Data - Sheet 1:\")\n",
    "display(sales_data_sheet1.head())\n",
    "\n",
    "print(\"\\nSales Data - Sheet 2:\")\n",
    "display(sales_data_sheet2.head())\n",
    "\n",
    "# Combine both sheets into a single DataFrame\n",
    "sales_data = pd.concat([sales_data_sheet1, sales_data_sheet2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "sales_data.to_csv('sales_data.csv', index=False)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined Sales Data:\")\n",
    "display(sales_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bbc73-a38c-48a8-a835-65aaed728830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Temperature data\n",
    "raw_temp_data_2021 = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Notebook\\temperature-madrid-2021.xlsx\"\n",
    "raw_temp_data_2022 = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Notebook\\temperature-madrid-2022.xlsx\"\n",
    "\n",
    "temp_data_2021 = pd.read_excel(raw_temp_data_2021)\n",
    "temp_data_2022 = pd.read_excel(raw_temp_data_2022)\n",
    "\n",
    "# Explore the DataFrame\n",
    "display(sales_data.head())\n",
    "display(temp_data_2021.head())      \n",
    "display(temp_data_2022.head())      \n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Sales Data:\")\n",
    "print(sales_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in 2021 Temperature Data:\")\n",
    "print(temp_data_2021.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in 2022 Temperature Data:\")\n",
    "print(temp_data_2022.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8c0e9-bed1-4aea-baf8-4126c12d3c42",
   "metadata": {},
   "source": [
    "### **2. Drop redundant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc6dec-eeee-4227-b434-736ef11002ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns from 2021 temperature data\n",
    "temp_data_2021_cleaned = temp_data_2021.drop(columns=['snow', 'wpgt', 'tsun'], errors='ignore')\n",
    "\n",
    "# Drop redundant columns from 2022 temperature data\n",
    "temp_data_2022_cleaned = temp_data_2022.drop(columns=['snow', 'wpgt', 'tsun'], errors='ignore')\n",
    "\n",
    "# Display the cleaned DataFrames\n",
    "print(\"Cleaned 2021 Temperature Data:\")\n",
    "display(temp_data_2021_cleaned.head())\n",
    "\n",
    "print(\"Cleaned 2022 Temperature Data:\")\n",
    "display(temp_data_2022_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81eff13-5f94-491b-ad51-dbff9d1ddcdb",
   "metadata": {},
   "source": [
    "### **3. (a) Rename the columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38166a18-2164-467a-94d3-e2073247ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename columns\n",
    "def rename_columns(df, column_mapping):\n",
    "    \"\"\"\n",
    "    Renames columns in the DataFrame based on the provided column mapping.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to rename columns in.\n",
    "    column_mapping (dict): A dictionary mapping old column names to new column names.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    return df.rename(columns=column_mapping)\n",
    "\n",
    "# Define column mappings for sales data and temperature data\n",
    "sales_column_mapping = {\n",
    "    'Province': 'Province',\n",
    "    'Town/City': 'Town_City',\n",
    "    'Post code': 'Postcode',\n",
    "    'Company code': 'Company_Code',\n",
    "    'Petrol station code': 'Petrol_Station_Code',\n",
    "    'Petrol station': 'Petrol_Station_Name',\n",
    "    'Date': 'Date',\n",
    "    'Sold units': 'Sold_Units'\n",
    "}\n",
    "\n",
    "temperature_column_mapping = {\n",
    "    'date': 'Date',\n",
    "    'tavg': 'Avg_Temp_Celsius',\n",
    "    'tmin': 'Min_Temp_Celsius',\n",
    "    'tmax': 'Max_Temp_Celsius',\n",
    "    'prcp': 'Total_Precipitation_mm',\n",
    "    'wdir': 'Avg_Wind_Direction_deg',\n",
    "    'wspd': 'Avg_Wind_Speed_kph',\n",
    "    'pres': 'Avg_Sea_Level_Pressure_hpa'\n",
    "}\n",
    "\n",
    "# Apply the renaming function to each dataset\n",
    "sales_data = rename_columns(sales_data, sales_column_mapping)\n",
    "temp_data_2021_cleaned = rename_columns(temp_data_2021_cleaned, temperature_column_mapping)\n",
    "temp_data_2022_cleaned = rename_columns(temp_data_2022_cleaned, temperature_column_mapping)\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(\"Renamed Sales Data:\")\n",
    "display(sales_data.head())\n",
    "\n",
    "print(\"Renamed 2021 Temperature Data:\")\n",
    "display(temp_data_2021_cleaned.head())\n",
    "\n",
    "print(\"Renamed 2022 Temperature Data:\")\n",
    "display(temp_data_2022_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eecbd6a-4d3f-4a99-aa92-576272a53988",
   "metadata": {},
   "source": [
    "### **3. (b) Ensure the 'date' column is correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539b5bb-c4cf-4f09-85f2-c8a5820f5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the Sales Data 'Date' column and overwrite it with the formatted string\n",
    "sales_data['Date'] = pd.to_datetime(sales_data['Date'], errors='coerce')\n",
    "\n",
    "# For the temperature data, explicitly specify the format\n",
    "temp_data_2021_cleaned['Date'] = pd.to_datetime(temp_data_2021_cleaned['Date'], errors='coerce')\n",
    "temp_data_2022_cleaned['Date'] = pd.to_datetime(temp_data_2022_cleaned['Date'], errors='coerce')\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Sales Data Date Range:\", sales_data['Date'].min(), \"-\", sales_data['Date'].max())\n",
    "print(\"2021 Temperature Data Date Range:\", temp_data_2021_cleaned['Date'].min(), \"-\", temp_data_2021_cleaned['Date'].max())\n",
    "print(\"2022 Temperature Data Date Range:\", temp_data_2022_cleaned['Date'].min(), \"-\", temp_data_2022_cleaned['Date'].max())\n",
    "\n",
    "# Display the first few rows of the updated Date column\n",
    "print(sales_data['Date'].dt.strftime('%d/%m/%Y').head())\n",
    "print(temp_data_2021_cleaned['Date'].dt.strftime('%d/%m/%Y').head())\n",
    "print(temp_data_2022_cleaned['Date'].dt.strftime('%d/%m/%Y').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30045ee-f4da-4f68-9af4-ec66075fd816",
   "metadata": {},
   "source": [
    "### **4. Save the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5e083-a022-4f08-8e90-b7a878f0ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder where you want to save the files\n",
    "output_folder = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Clean_Data\"  # This goes one level up and creates 'Clean_Data'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Save the cleaned DataFrames in the specified folder\n",
    "sales_data.to_csv(os.path.join(output_folder, \"cleaned_sales_data.csv\"), index=False)\n",
    "temp_data_2021_cleaned.to_csv(os.path.join(output_folder, \"cleaned_temperature_data_2021.csv\"), index=False)\n",
    "temp_data_2022_cleaned.to_csv(os.path.join(output_folder, \"cleaned_temperature_data_2022.csv\"), index=False)\n",
    "\n",
    "print(f\"DataFrames have been saved to {output_folder}\")\n",
    "\n",
    "# Load the cleaned dataset to verify\n",
    "\n",
    "print(\"Cleaned Sales Data:\")\n",
    "display(sales_data.head())\n",
    "\n",
    "print(\"\\nCleaned 2021 Temperature Data:\")\n",
    "display(temp_data_2021_cleaned.head())\n",
    "\n",
    "print(\"\\nCleaned 2022 Temperature Data:\")\n",
    "display(temp_data_2022_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b778ce-266c-4ac5-b455-32ed7f4410d6",
   "metadata": {},
   "source": [
    "### **5. Merge the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb3c7d-3539-4d6a-80c8-f5311a9ca9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 2021 and 2022 temperature datasets\n",
    "merged_temp_data = pd.concat([temp_data_2021_cleaned, temp_data_2022_cleaned], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_temp_data.to_csv('merged_temp_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the merged temperature data\n",
    "print(\"Merged Temperature Data:\")\n",
    "display(merged_temp_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c54a6-232e-453e-a07c-b4ffe86ba140",
   "metadata": {},
   "source": [
    "### **6. Data quality checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8140a6-3a08-40d7-93b2-5fc88085bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "# Group the data by year and month using the valid dates\n",
    "for (year, month), group in merged_temp_data.groupby([merged_temp_data['Date'].dt.year, merged_temp_data['Date'].dt.month]):\n",
    "    # Cast year and month to int (they should be integers already, but this ensures it)\n",
    "    year = int(year)\n",
    "    month = int(month)\n",
    "    # Determine the expected days in this month\n",
    "    expected_days = set(range(1, calendar.monthrange(year, month)[1] + 1))\n",
    "    # Extract the unique day numbers from the 'Date' column in the group\n",
    "    actual_days = set(group['Date'].dt.day.unique())\n",
    "    # Find any missing days\n",
    "    missing_days = expected_days - actual_days\n",
    "    \n",
    "    # Print the result for this month\n",
    "    if missing_days:\n",
    "        print(f\"For {year}-{month:02d}, missing days: {sorted(missing_days)}\")\n",
    "    else:\n",
    "        print(f\"For {year}-{month:02d}, all days are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb3c30-31b1-42f1-905f-ce4352488236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the left join between Sales and Temperature DataFrames\n",
    "merged_data = pd.merge(sales_data, merged_temp_data, how='left', on = 'Date')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc18c88-bbe5-402b-bcb8-c42f09d1d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with missing or invalid dates\n",
    "missing_dates = merged_data[merged_data['Date'].isnull()]\n",
    "print(f\"Number of rows with missing dates: {len(missing_dates)}\")\n",
    "display(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b1ada-efed-494c-8c21-52995e881101",
   "metadata": {},
   "source": [
    "### **7. Save filtered data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6492b8a-bc52-45fa-b1ef-11f6923b50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting dataset to a CSV file\n",
    "output_folder = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Merged_Data\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "merged_data.to_csv(os.path.join(output_folder, \"merged_data.csv\"), index=False)\n",
    "print(f\"Merged dataset saved to: {os.path.join(output_folder, 'merged_data.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf5ac3-2fe1-4a1c-9d6c-f0e00ae40aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the reduced DataFrame\n",
    "print(\"Reduced DataFrame:\")\n",
    "display(merged_data.head())\n",
    "\n",
    "# Ensure 'date' column is in datetime format\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'], errors='coerce')\n",
    "\n",
    "# Extract and display unique years from 'date'\n",
    "unique_years = merged_data['Date'].dt.year.unique()\n",
    "print(f\"\\nUnique years in the data: {unique_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a20db2-80fc-4a8a-899a-abfcadd18d6c",
   "metadata": {},
   "source": [
    "### **Key Observations:**\n",
    "\n",
    "#### Data Integrity & Completeness\n",
    "- All months for 2021 and 2022 have complete daily records, ensuring consistency in time-series analysis.\n",
    "- The sales dataset has no missing dates, and all sales records align with existing temperature data, ensuring a strong foundation for correlation analysis.\n",
    "\n",
    "#### Merging & Alignment of Data\n",
    "- The left join between sales and temperature data ensures that every sales record has corresponding weather data, preventing loss of critical information.\n",
    "- Unique years extracted (2021 and 2022) confirm that the dataset is well-structured for year-over-year trend comparisons.\n",
    "- The merged dataset has been saved successfully, ensuring further analysis can be performed without reprocessing raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586aa78-91c8-4b79-848f-4a5296feaee7",
   "metadata": {},
   "source": [
    "## **B. Filter the Merged Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc94ab-a962-4b01-bd2f-04b28b6edec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update the province based on postcode\n",
    "def update_province(row):\n",
    "    if str(row['Postcode']).startswith('28'):\n",
    "        return 'Madrid'\n",
    "    elif str(row['Postcode']).startswith('45'):\n",
    "        return 'Northern Toledo'\n",
    "    elif str(row['Postcode']).startswith('19'):\n",
    "        return 'Guadalajara'\n",
    "    else:\n",
    "        return row['Province']  # Retain the original value if no match\n",
    "\n",
    "merged_data['Province'] = merged_data.apply(update_province, axis=1)\n",
    "\n",
    "# Rename 'Desconocido' in the province column to 'Guadalajara'\n",
    "merged_data['Province'] = merged_data['Province'].replace('Desconocido', 'Guadalajara')\n",
    "\n",
    "# Ensure all postcodes in merged_data are strings\n",
    "merged_data['Postcode'] = merged_data['Postcode'].astype(str)\n",
    "\n",
    "# Add leading zeroes if necessary (e.g., make all postcodes 5 digits)\n",
    "merged_data['Postcode'] = merged_data['Postcode'].str.zfill(5)\n",
    "\n",
    "# Sorting data by postcode.\n",
    "filtered_data = merged_data[merged_data['Postcode'].astype(str).str.startswith(('28', '19', '45'))]\n",
    "\n",
    "display(filtered_data['Postcode'].unique())\n",
    "display(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7849fe-ad30-4b83-a8b3-408f92e30f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411330da-dcaa-400e-ae93-2adfaa5fec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import googlemaps\n",
    "import os\n",
    "\n",
    "# Initialise Google Maps API\n",
    "API_KEY = \"AIzaSyDlNwDamGjBwmawbvdG7On2m7TnF7KimAU\"\n",
    "gmaps = googlemaps.Client(key=API_KEY)\n",
    "\n",
    "# Madrid Central Coordinates\n",
    "central_madrid_coords = (40.4168, -3.7038)\n",
    "\n",
    "# Function to Fetch Coordinates\n",
    "def fetch_coordinates(postcode):\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(f\"{postcode}, Spain\")\n",
    "        if geocode_result:\n",
    "            location = geocode_result[0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {postcode}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to Calculate Distance\n",
    "def calculate_distance(row):\n",
    "    try:\n",
    "        store_coords = (row['Latitude'], row['Longitude'])\n",
    "        return geodesic(store_coords, central_madrid_coords).km\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Ensure 'Postcode' exists before proceeding\n",
    "if 'Postcode' in filtered_data.columns:\n",
    "    \n",
    "    # Fetch Coordinates and Assign to New Columns\n",
    "    filtered_data[['Latitude', 'Longitude']] = filtered_data['Postcode'].apply(\n",
    "        lambda x: pd.Series(fetch_coordinates(x))\n",
    "    )\n",
    "    \n",
    "    # Calculate Distance and Assign\n",
    "    filtered_data['Distance_From_Madrid'] = filtered_data.apply(calculate_distance, axis=1)\n",
    "\n",
    "# Ensure 'Date' is a datetime type\n",
    "filtered_data['Date'] = pd.to_datetime(filtered_data['Date'], errors='coerce')\n",
    "\n",
    "# Filter Data by Distance (Within 65 km)\n",
    "filtered_data = filtered_data[filtered_data['Distance_From_Madrid'] <= 65]\n",
    "\n",
    "# Filter Data by Year\n",
    "filtered_data['Year'] = filtered_data['Date'].dt.year\n",
    "filtered_data = filtered_data[filtered_data['Year'].isin([2021, 2022])]\n",
    "\n",
    "# Save Processed File\n",
    "output_folder = r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Merged_Data\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"filtered_data_65km_2021_2022.xlsx\")\n",
    "filtered_data.to_excel(output_file, index=False, engine='openpyxl')\n",
    "print(f\"Filtered data saved to: {output_file}\")\n",
    "\n",
    "# Verify if Columns Exist\n",
    "print(\"Final Processed Data:\")\n",
    "print(filtered_data[['Postcode', 'Latitude', 'Longitude', 'Distance_From_Madrid']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489891a-8aae-4bc9-bebb-8ce0623587b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "print(\"Filtered Data:\")\n",
    "display(filtered_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40332503-be3d-4961-a3c6-235b098596d5",
   "metadata": {},
   "source": [
    "### **Key Observations:**\n",
    "\n",
    "#### Data Completeness & Structure\n",
    "- The filtered dataset contains `39,399 records` after removing entries outside the `65 km radius of Madrid` and keeping only data from 2021 and 2022.\n",
    "- `Latitude` and `Longitude` coordinates were successfully fetched for all petrol stations using `Google Maps API`.\n",
    "- All entries now include `Distance_From_Madrid`, which enables spatial analysis of urban vs. rural sales performance.\n",
    "- The dataset retains critical meteorological variables (temperature, precipitation, wind speed, and sea-level pressure) for weather impact analysis on sales trends.\n",
    "  \n",
    "#### Geospatial Insights\n",
    "- Majority of records fall within `Madrid`, `Guadalajara`, and `Northern Toledo`, as indicated by postcode filtering.\n",
    "- The distance calculation from Madrid's center `(40.4168, -3.7038)` allows segmentation into high-density urban vs. lower-density suburban and highway petrol stations.\n",
    "- Higher density of sales entries in central Madrid suggests that urban locations experience more frequent transactions, whereas rural stations have more sporadic sales.\n",
    "  \n",
    "#### Sales vs. Weather Trends\n",
    "- Temperature data is almost fully complete (`Avg_Temp_Celsius` has minor missing values), meaning we can conduct robust seasonal and extreme temperature impact analyses.\n",
    "- The dataset confirms that peak temperatures and sold units can be correlated, enabling the identification of demand spikes during heatwaves (35°C+).\n",
    "\n",
    "#### Data Cleaning & Standardisation\n",
    "- The province values have been corrected to reflect known regions based on postcodes (`Madrid`, `Guadalajara`, `Northern Toledo`).\n",
    "- Postcodes have been formatted as strings and standardised to `five-digit format` to maintain consistency.\n",
    "- Data was successfully filtered to exclude unnecessary locations, ensuring that all petrol stations in the dataset are relevant for Madrid-centric analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825bb4a0-d3e5-4c50-8133-68ceae52af53",
   "metadata": {},
   "source": [
    "## **C. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b5f92-41f1-48ac-a9c1-115f02d6e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93704572-4669-4e84-ac49-781423e26e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month for analysis\n",
    "filtered_data['Year'] = filtered_data['Date'].dt.year\n",
    "filtered_data['Month'] = filtered_data['Date'].dt.month\n",
    "filtered_data['Month_Name'] = filtered_data['Date'].dt.strftime('%B')\n",
    "\n",
    "# -------- 1. Hottest Month in Each Year -------- #\n",
    "hot_months = filtered_data.groupby(['Year', 'Month_Name'])['Max_Temp_Celsius'].mean().reset_index()\n",
    "hot_months = hot_months.loc[hot_months.groupby('Year')['Max_Temp_Celsius'].idxmax()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=hot_months, x='Year', y='Max_Temp_Celsius', hue='Month_Name')\n",
    "plt.title('Hottest Month in Each Year')\n",
    "plt.ylabel('Max Temperature (°C)')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title=\"Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482a72e-38b5-42d2-b8b2-60027d8fd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 2. Highest Sales by Month -------- #\n",
    "monthly_sales = filtered_data.groupby(['Year', 'Month_Name'])['Sold_Units'].sum().reset_index()\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(data=monthly_sales, x='Month_Name', y='Sold_Units', hue='Year')\n",
    "plt.title('Highest Sales by Month')\n",
    "plt.ylabel('Total Sold Units')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bcc01-d185-499d-b7e6-4cdc2ef4e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 3. Impact of Festivals on Sales -------- #\n",
    "# Load festival data (Ensure correct path)\n",
    "festival_df = pd.read_excel(r'C:\\Users\\hp\\Downloads\\festivals_holidays_Madrid.xlsx')\n",
    "\n",
    "# Handling festival date ranges\n",
    "festival_df[['Start_Date', 'End_Date']] = festival_df['Date'].str.split(' to ', expand=True)\n",
    "\n",
    "# Convert to datetime format\n",
    "festival_df['Start_Date'] = pd.to_datetime(festival_df['Start_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "festival_df['End_Date'] = pd.to_datetime(festival_df['End_Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Expand date ranges into individual dates\n",
    "expanded_dates = []\n",
    "for _, row in festival_df.iterrows():\n",
    "    if pd.notna(row['Start_Date']) and pd.notna(row['End_Date']):\n",
    "        expanded_dates.extend(pd.date_range(row['Start_Date'], row['End_Date']).tolist())\n",
    "\n",
    "# Create a DataFrame of all festival dates\n",
    "expanded_festival_df = pd.DataFrame({'Date': expanded_dates})\n",
    "\n",
    "# Merge sales data with expanded festival dates\n",
    "merged_festival_sales = filtered_data.merge(expanded_festival_df, on='Date', how='inner')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=filtered_data, x='Date', y='Sold_Units', label=\"Overall Sales\", alpha=0.5)\n",
    "sns.scatterplot(data=merged_festival_sales, x='Date', y='Sold_Units', color='red', label=\"Festival Days\", s=100)\n",
    "plt.title('Impact of Festivals on Sales')\n",
    "plt.ylabel('Sold Units')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc625db8-8abc-4380-986d-b890a4a5ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 4. Sales Trends Over Time -------- #\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=filtered_data, x='Date', y='Sold_Units', hue='Year', palette='coolwarm')\n",
    "plt.title('Sales Trends Over Time')\n",
    "plt.ylabel('Sold Units')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(title=\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ddcb7-20e0-49ee-a56f-f747561029dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 5. Temperature vs. Sales Analysis -------- #\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=filtered_data, x='Max_Temp_Celsius', y='Sold_Units', alpha=0.5)\n",
    "sns.regplot(data=filtered_data, x='Max_Temp_Celsius', y='Sold_Units', scatter=False, color='red')\n",
    "plt.title('Temperature vs. Sales Analysis')\n",
    "plt.ylabel('Sold Units')\n",
    "plt.xlabel('Max Temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb3887-c94f-46d8-8f07-319e9d7b3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install folium\n",
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5c324-7fbe-4510-8352-8d0327b3ba67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spatial Analysis: Map sales performance by location (e.g., stores or regions).\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Load the filtered data (Ensure this DataFrame is available)\n",
    "df = filtered_data.copy()\n",
    "\n",
    "# Ensure Latitude and Longitude are numeric\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "df.dropna(subset=['Latitude', 'Longitude', 'Sold_Units'], inplace=True)\n",
    "\n",
    "# Create a base map centered in Madrid\n",
    "madrid_map = folium.Map(location=[40.4168, -3.7038], zoom_start=10, tiles=\"cartodb positron\")\n",
    "\n",
    "# -------- 1. Choropleth Map: Aggregate sales per location -------- #\n",
    "sales_by_location = df.groupby(['Latitude', 'Longitude'], as_index=False)['Sold_Units'].sum()\n",
    "\n",
    "choropleth_layer = folium.FeatureGroup(name=\"Sales Heatmap\")\n",
    "for _, row in sales_by_location.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=row['Sold_Units'] / max(sales_by_location['Sold_Units']) * 10,  # Scale marker size\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_color=\"blue\",\n",
    "        fill_opacity=0.6,\n",
    "        popup=f\"Sales: {row['Sold_Units']}\"\n",
    "    ).add_to(choropleth_layer)\n",
    "\n",
    "choropleth_layer.add_to(madrid_map)\n",
    "\n",
    "# -------- 2. Marker Cluster Map -------- #\n",
    "marker_cluster = MarkerCluster(name=\"Petrol Stations\").add_to(madrid_map)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=f\"Sold Units: {row['Sold_Units']}\\nPostcode: {row['Postcode']}\",\n",
    "        icon=folium.Icon(color=\"green\"),\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Add Layer Control\n",
    "folium.LayerControl().add_to(madrid_map)\n",
    "\n",
    "# Save the map\n",
    "madrid_map.save(\"sales_spatial_analysis.html\")\n",
    "madrid_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8d28c-5be3-426d-bc5e-1e27eaa5ae17",
   "metadata": {},
   "source": [
    "### **Key Observations:**\n",
    "\n",
    "#### Seasonal and Temperature Impact on Sales\n",
    "- Sales are `highly temperature-driven`, with demand rising significantly beyond `35°C`.\n",
    "- `July and August` consistently recorded the `highest sales`, aligning with peak summer heat.\n",
    "- `2022 saw higher maximum temperatures` compared to 2021, leading to an increase in sales.\n",
    "\n",
    "#### Festivals and Event-Driven Demand\n",
    "- `Festivals drive notable spikes in sales`, especially during July and August.\n",
    "- Peaks in sales `align with major public holidays and events`, emphasising the need for `event-based stock planning`.\n",
    "\n",
    "#### Long-Term Sales Trends\n",
    "- `Distinct seasonal patterns` emerge, with `sales peaking in summer and dropping in winter`.\n",
    "- `2022 recorded higher overall sales than 2021`, possibly due to `warmer temperatures and increased consumer activity`.\n",
    "- `Weekly sales spikes suggest demand surges during weekends`.\n",
    "\n",
    "#### Spatial Analysis: Urban vs. Rural Demand\n",
    "- `Madrid city and suburban areas` have the `highest sales density`, reinforcing `urban dominance`.\n",
    "- `Petrol stations on highways and in tourist-heavy rural areas` also experience significant sales.\n",
    "- The `sales heatmap highlights concentrated demand zones`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554f881-ab44-4daa-8e08-929e979dfc6f",
   "metadata": {},
   "source": [
    "## **D. Consumer Behaviour Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361de27-a851-418e-b835-ebf0f8362d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_excel(r\"C:\\Users\\hp\\OneDrive\\Documents\\GitHub\\LSE_Gaea-AI-Project\\Merged_Data\\filtered_data_65km_2021_2022.xlsx\")\n",
    "display(data_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8d52a-e05c-4d55-9c51-dedafdf829a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is in datetime format\n",
    "data_2['Date'] = pd.to_datetime(data_2['Date'])\n",
    "\n",
    "# Group by 'Petrol_Station_Code', summing the total 'Sold_Units'\n",
    "sales_summary = data_2.groupby(['Petrol_Station_Code', 'Petrol_Station_Name', 'Postcode'])['Sold_Units'].sum().reset_index()\n",
    "\n",
    "# Sort to get top 10 highest-selling locations\n",
    "top_10_selling = sales_summary.sort_values(by='Sold_Units', ascending=False).head(10)\n",
    "\n",
    "# Sort to get 10 locations with the lowest sales\n",
    "lowest_10_selling = sales_summary.sort_values(by='Sold_Units', ascending=True).head(10)\n",
    "\n",
    "# Display the results\n",
    "display(top_10_selling)\n",
    "\n",
    "display(lowest_10_selling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6f37e-3575-4066-9ba1-bec464973149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries.\n",
    "import matplotlib.dates as mdates\n",
    "# Filter the data for April - July 2022\n",
    "filtered_data = data_2[(data_2['Date'] >= '2022-04-01') & (data_2['Date'] <= '2022-07-01')]\n",
    "\n",
    "# Find the top 3 petrol stations based on total 'Sold_Units'\n",
    "top_3_locations = filtered_data.groupby('Petrol_Station_Code')['Sold_Units'].sum().nlargest(3).index\n",
    "\n",
    "# Filter data to only include the top 3 stations\n",
    "top_3_data = filtered_data[filtered_data['Petrol_Station_Code'].isin(top_3_locations)]\n",
    "\n",
    "# Group by 'Date' and 'Petrol_Station_Code', then sum 'Sold_Units'\n",
    "daily_sales = top_3_data.groupby(['Date', 'Petrol_Station_Code'])['Sold_Units'].sum().reset_index()\n",
    "\n",
    "# Create a mapping dictionary from station codes to names\n",
    "station_name_mapping = data_2.set_index('Petrol_Station_Code')['Petrol_Station_Name'].to_dict()\n",
    "\n",
    "# Add 'Petrol_Station_Name' to daily_sales using the mapping\n",
    "daily_sales['Petrol_Station_Name'] = daily_sales['Petrol_Station_Code'].map(station_name_mapping)\n",
    "\n",
    "# Group by 'Date' and calculate the average max temperature\n",
    "tmax_data = top_3_data.groupby('Date')['Max_Temp_Celsius'].mean().reset_index()\n",
    "tmax_data.rename(columns={'Max_Temp_Celsius': 'tmax'}, inplace=True)\n",
    "\n",
    "# Convert Date column to datetime\n",
    "daily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n",
    "tmax_data['Date'] = pd.to_datetime(tmax_data['Date'])\n",
    "\n",
    "# Filter data for June 2022 only\n",
    "daily_sales_june = daily_sales[daily_sales['Date'].dt.month == 6]\n",
    "tmax_data_june = tmax_data[tmax_data['Date'].dt.month == 6]\n",
    "\n",
    "# Ensure there are unique stations for the palette\n",
    "unique_stations = daily_sales_june['Petrol_Station_Name'].nunique()\n",
    "custom_palette = sns.color_palette(\"husl\", unique_stations if unique_stations > 0 else 1)\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot Sold Units (Line Plot)\n",
    "sns.lineplot(\n",
    "    data=daily_sales_june,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    hue='Petrol_Station_Name',\n",
    "    marker='o',\n",
    "    palette=custom_palette,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "# Customize first y-axis\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Sold Units', fontsize=12, color='black')\n",
    "ax1.set_title('Sold Units Per Day for Top 3 Locations (June 2022)', fontsize=16)\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Format x-axis for better readability\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%a %d-%b'))\n",
    "ax1.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add a second y-axis for tmax\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    tmax_data_june['Date'],\n",
    "    tmax_data_june['tmax'],\n",
    "    color='black',\n",
    "    linestyle='--',\n",
    "    marker='s',\n",
    "    label='Max Temp'\n",
    ")\n",
    "\n",
    "# Customize second y-axis\n",
    "ax2.set_ylabel('Temperature (°C)', fontsize=12, color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Adjust petrol station legend (inside the plot, upper left)\n",
    "ax1.legend(loc='upper left', bbox_to_anchor=(0.02, 0.98), title=\"Petrol Station\", frameon=True)\n",
    "\n",
    "# Adjust temperature legend (outside the plot, below the title)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(0.98, 0.85), title=\"\", frameon=True)\n",
    "\n",
    "# Grid, layout, and save\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de04de-9a15-4ea2-b8c4-689208dad017",
   "metadata": {},
   "source": [
    "## **Key Observations from Ice Sales and Temperature Trends in June 2022**\n",
    "\n",
    "#### Sales Surge with Rising Temperatures\n",
    "- The `highest peaks in sales` coincide with extreme temperature spikes (above 35°C).\n",
    "- The `three highest peaks (June 11th, June 18th, June 25th)` align with significant heatwaves.\n",
    "- Petrol stations experience a clear increase in demand as temperatures approach and exceed 35°C.\n",
    "\n",
    "#### Top Performing Petrol Stations\n",
    "- `Hipódromo and Valdemorillo` show the highest sales spikes, indicating strong demand in these locations.\n",
    "- `Concha Espina` maintains steady but lower sales, suggesting less temperature-driven demand compared to other stations.\n",
    "- The `peak on June 11th and June 18th` was `dominated by Valdemorillo`, while Hipódromo took the lead on June 25th.\n",
    "\n",
    "#### Influence of Festivals and Events\n",
    "- The `San Juan Festival (June 23rd-24th)` coincides with a surge in sales across all locations, confirming the impact of cultural events on ice demand.\n",
    "- Other `local gatherings and summer celebrations in mid-June` likely contribute to the sales peaks, particularly in suburban and semi-rural areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dee7e0-5a7b-4e35-b8ad-595bf35c4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format\n",
    "data_2[\"Date\"] = pd.to_datetime(data_2[\"Date\"], dayfirst=True)\n",
    "\n",
    "# Filter for the specific dates\n",
    "target_dates = [\"2022-06-17\"]\n",
    "df_filtered = data_2[data_2[\"Date\"].isin(pd.to_datetime(target_dates))]\n",
    "\n",
    "# Group by Petrol Station Code, Name, and Postcode, summing Sold Units\n",
    "top_selling = (\n",
    "    df_filtered.groupby([\"Petrol_Station_Code\", \"Petrol_Station_Name\", \"Postcode\",\"Date\",\"Town_City\"])\n",
    "    [\"Sold_Units\"].sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"Date\", \"Sold_Units\"], ascending=[True, False])\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# Display the top-selling locations\n",
    "display(top_selling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5549e21-6f12-449c-a476-123e484fb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format\n",
    "filtered_data[\"Date\"] = pd.to_datetime(filtered_data[\"Date\"], dayfirst=True)\n",
    "\n",
    "# Filter for Summer 2022 (June 1 - September 1) and Petrol Station Codes 932 and 935\n",
    "filtered_data_2 = data_2[(data_2['Date'] >= '2022-06-01') & (data_2['Date'] <= '2022-09-01')]\n",
    "filtered_28 = filtered_data_2[filtered_data_2['Petrol_Station_Code'].isin([932, 935])]\n",
    "\n",
    "# Group by 'Date' and 'Petrol_Station_Code', summing 'Sold_Units'\n",
    "daily_sales_28 = filtered_28.groupby(['Date', 'Petrol_Station_Code'])['Sold_Units'].sum().reset_index()\n",
    "\n",
    "# Create the line plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=daily_sales_28,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    hue='Petrol_Station_Code',\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Daily Sold Units - 2 top selling places in Madrid during Summer Story Festival', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sold Units', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Format x-axis to display dates\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%a %d-%b'))\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('sold_units_July.png', dpi=300)  # Save with high resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557e08c-5a16-4e91-9782-ef35f730f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Summer 2022 (28007)\n",
    "filtered_data_2 = data_2[(data_2['Date'] >= '2022-06-01') & (data_2['Date'] <= '2022-09-01')]\n",
    "\n",
    "# Filter for postcode 28041\n",
    "filtered_932 = filtered_data_2[filtered_data_2['Petrol_Station_Code'] == 932]\n",
    "\n",
    "# Group by 'Date' and sum 'Sold_Units'\n",
    "daily_sales_932 = filtered_932.groupby('Date')['Sold_Units'].sum().reset_index()\n",
    "# Define a color for the line\n",
    "custom_palette = ['#1f77b4']  # Use a single color since we only have one postcode\n",
    "\n",
    "# Create the line plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=daily_sales_932,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=custom_palette[0]  # Use a single color since there's only one Postcode\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Daily Sold Units - Calle del Doctor Esquerdo (Summer 2022)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sold Units', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Format x-axis to display dates\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%a %d-%b'))\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))  # Set interval to every 3 days\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('sold_units_July.png', dpi=300)  # Save with high resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e96e5-399c-45a8-86d3-30270c7ac680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "sold_units_color = '#1f77b4'  # Blue for Sold Units\n",
    "tmax_color = '#ff7f0e'  # Orange for Max Temperature\n",
    "\n",
    "# Create the figure and primary axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot Sold Units\n",
    "sns.lineplot(\n",
    "    data=filtered_932,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=sold_units_color,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "# Customize primary axis\n",
    "ax1.set_title('Daily Sold Units & Max Temp - Calle del Doctor Esquerdo (Summer 2022)', fontsize=16)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Sold Units', fontsize=12, color=sold_units_color)\n",
    "ax1.tick_params(axis='y', labelcolor=sold_units_color)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary y-axis for Max Temperature\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=filtered_932,\n",
    "    x='Date',\n",
    "    y='Max_Temp_Celsius',  # Corrected column name\n",
    "    marker='s',\n",
    "    color=tmax_color,\n",
    "    linestyle='dashed',\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "# Customize secondary axis\n",
    "ax2.set_ylabel('Max Temperature (°C)', fontsize=12, color=tmax_color)\n",
    "ax2.tick_params(axis='y', labelcolor=tmax_color)\n",
    "\n",
    "# Format x-axis to display dates\n",
    "ax1.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%a %d-%b'))\n",
    "ax1.xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))  # Set interval to every 7 days\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('sold_units_tmax_July.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf349daf-baec-4add-be6c-eda33b3d222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features to the graph.\n",
    "# Define colors\n",
    "sold_units_color = '#1f77b4'  # Blue for Sold Units\n",
    "tmax_color = '#ff7f0e'  # Orange for tmax\n",
    "\n",
    "# Create the figure and primary axis with a larger size\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))  # Increased figure size\n",
    "\n",
    "# Plot Sold Units\n",
    "sns.lineplot(\n",
    "    data=filtered_932,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=sold_units_color,\n",
    "    ax=ax1,\n",
    "    label='Sold Units'  # Add label for Sold Units\n",
    ")\n",
    "\n",
    "# Customize primary axis\n",
    "ax1.set_title('Daily Sold Units & Max Temp - Calle del Doctor Esquerdo (Summer 2022)', fontsize=16)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Sold Units', fontsize=12, color=sold_units_color)\n",
    "ax1.tick_params(axis='y', labelcolor=sold_units_color)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary y-axis for tmax\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=filtered_932,\n",
    "    x='Date',\n",
    "    y='Max_Temp_Celsius',\n",
    "    marker='s',\n",
    "    color=tmax_color,\n",
    "    linestyle='dashed',\n",
    "    ax=ax2,\n",
    "    label='Max Temperature'  # Add label for Max Temperature\n",
    ")\n",
    "\n",
    "# Customize secondary axis\n",
    "ax2.set_ylabel('tmax (°C)', fontsize=12, color=tmax_color)\n",
    "ax2.tick_params(axis='y', labelcolor=tmax_color)\n",
    "\n",
    "# Manually set x-ticks (date labels)\n",
    "ax1.set_xticks(filtered_932['Date'][::5])  # Show every 5th date (adjust the step as needed)\n",
    "ax1.set_xticklabels(filtered_932['Date'][::5].dt.strftime('%a %d-%b'), rotation=60, ha='right')  # Rotate at 60 degrees and right-align\n",
    "\n",
    "# Adjust layout for more space\n",
    "plt.subplots_adjust(bottom=0.2)  # Increase bottom margin to ensure labels fit\n",
    "\n",
    "# Add legends for both axes\n",
    "ax1.legend(loc='upper left', title=\"\")\n",
    "ax2.legend(loc='upper right', title=\"\")\n",
    "\n",
    "# Save and show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.savefig('doctor escquerdo.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b4620-5424-4e97-af06-d6437dcd86c7",
   "metadata": {},
   "source": [
    "Checking a location close to the Ciudad del Rock to see the sale pattern through the summer of 2022. Postcodes are taken from a map created in Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb72df-0406-4471-afd3-ead48e0c17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for postcode 28540\n",
    "filtered_stations = data_2[data_2[\"Postcode\"] == 28540][[\"Postcode\", \"Petrol_Station_Code\", \"Petrol_Station_Name\"]].drop_duplicates()\n",
    "\n",
    "# Display the result\n",
    "print(filtered_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dcdad-cf98-4bb9-a44d-695c73c5cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Summer 2022 (28540)\n",
    "filtered_data_2 = data_2[(data_2['Date'] >= '2022-06-01') & (data_2['Date'] <= '2022-09-01')]\n",
    "\n",
    "# Filter for postcode 28540\n",
    "filtered_1688 = filtered_data_2[filtered_data_2['Petrol_Station_Code'] == 1688]\n",
    "\n",
    "# Group by 'Date' and sum 'Sold_Units'\n",
    "daily_sales_1688 = filtered_1688.groupby('Date').agg({'Sold_Units': 'sum', 'Max_Temp_Celsius': 'max'}).reset_index()\n",
    "# Define a color for the line\n",
    "custom_palette = ['#1f77b4']  # Use a single color since we only have one postcode\n",
    "\n",
    "# Create the line plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=daily_sales_1688,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=custom_palette[0]  # Use a single color since there's only one Postcode\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Daily Sold Units - Ciudad del Rock (Summer 2022)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sold Units', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Format x-axis to display dates\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%a %d-%b'))\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))  # Set interval to every 3 days\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2d9a6-896b-4573-8200-31d9b739a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "custom_palette = ['#1f77b4']  # Blue for Sold_Units\n",
    "tmax_color = '#ff7f0e'  # Orange for tmax\n",
    "\n",
    "# Create the figure and primary axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot Sold Units\n",
    "sns.lineplot(\n",
    "    data=filtered_1688,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=custom_palette[0],\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "# Customize primary axis\n",
    "ax1.set_title('Daily Sold Units & Max Temp - Ciudad del Rock (Summer 2022)', fontsize=16)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Sold Units', fontsize=12, color=custom_palette[0])\n",
    "ax1.tick_params(axis='y', labelcolor=custom_palette[0])\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary y-axis for tmax\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=filtered_1688,\n",
    "    x='Date',\n",
    "    y='Max_Temp_Celsius',\n",
    "    marker='s',\n",
    "    color=tmax_color,\n",
    "    linestyle='dashed',\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "# Customize secondary axis\n",
    "ax2.set_ylabel('tmax (°C)', fontsize=12, color=tmax_color)\n",
    "ax2.tick_params(axis='y', labelcolor=tmax_color)\n",
    "\n",
    "# Format x-axis to display dates\n",
    "ax1.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%a %d-%b'))\n",
    "ax1.xaxis.set_major_locator(plt.matplotlib.dates.DayLocator(interval=7))  # Set interval to every 7 days\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('sold_units_tmax_July.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85f06-18fb-4da3-9418-6bb171b84b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new features to the graph.\n",
    "# Define colors\n",
    "sold_units_color = '#1f77b4'  # Blue for Sold Units\n",
    "tmax_color = '#ff7f0e'  # Orange for tmax\n",
    "\n",
    "# Create the figure and primary axis with a larger size\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))  # Increased figure size\n",
    "\n",
    "# Plot Sold Units\n",
    "sns.lineplot(\n",
    "    data=filtered_1688,\n",
    "    x='Date',\n",
    "    y='Sold_Units',\n",
    "    marker='o',\n",
    "    color=sold_units_color,\n",
    "    ax=ax1,\n",
    "    label='Sold Units'  # Add label for Sold Units\n",
    ")\n",
    "\n",
    "# Customize primary axis\n",
    "ax1.set_title('Daily Sold Units & Max Temp - Calle del Doctor Esquerdo (Summer 2022)', fontsize=16)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Sold Units', fontsize=12, color=sold_units_color)\n",
    "ax1.tick_params(axis='y', labelcolor=sold_units_color)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary y-axis for tmax\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=filtered_1688,\n",
    "    x='Date',\n",
    "    y='Max_Temp_Celsius',\n",
    "    marker='s',\n",
    "    color=tmax_color,\n",
    "    linestyle='dashed',\n",
    "    ax=ax2,\n",
    "    label='Max Temperature'  # Add label for Max Temperature\n",
    ")\n",
    "\n",
    "# Customize secondary axis\n",
    "ax2.set_ylabel('Temperature (°C)', fontsize=12, color=tmax_color)\n",
    "ax2.tick_params(axis='y', labelcolor=tmax_color)\n",
    "\n",
    "# Manually set x-ticks (date labels)\n",
    "ax1.set_xticks(filtered_1688['Date'][::5])  # Show every 5th date (adjust the step as needed)\n",
    "ax1.set_xticklabels(filtered_1688['Date'][::5].dt.strftime('%a %d-%b'), rotation=60, ha='right')  # Rotate at 60 degrees and right-align\n",
    "\n",
    "# Adjust layout for more space\n",
    "plt.subplots_adjust(bottom=0.2)  # Increase bottom margin to ensure labels fit\n",
    "\n",
    "# Add legends for both axes\n",
    "ax1.legend(loc='upper left', title=\"\")\n",
    "ax2.legend(loc='upper right', title=\"\")\n",
    "\n",
    "# Save and show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.savefig('doctor escquerdo.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7266e-5d7b-4745-9479-f09cecd99ee7",
   "metadata": {},
   "source": [
    "### **Key Observations:**\n",
    "\n",
    "#### Sales and Temperature Correlation\n",
    "\n",
    "- Petrol sales appear to peak during hotter periods, particularly in summer months like `June`, `July`, and `August`.\n",
    "- A strong correlation is observed between temperature spikes and increased sales.\n",
    "  \n",
    "#### Top 3 Petrol Stations Performance (June 2022)\n",
    "\n",
    "- Stations like `HIPÓDROMO`, `VALDEMORILLO`, and `CONCHA ESPINA` exhibit notable peaks in sales corresponding to rising temperatures.\n",
    "- Major spikes in sales occur between `June 10-12 and June 16-18`, aligning with the `hottest days`.\n",
    "\n",
    "#### Impact of Festivals on Sales\n",
    "\n",
    "- Locations near festival sites, such as `Ciudad del Rock` and `Calle del Doctor Esquerdo`, experience significant demand surges on event days.\n",
    "- The `Summer Story Festival (June 2022)` resulted in sharp spikes in ice purchases.\n",
    "- Sales were highest on `June 17` and `August 5`, corresponding to major festival events.\n",
    "\n",
    "#### Daily Sales Trends at Key Locations\n",
    "\n",
    "- Petrol stations at Calle del Doctor Esquerdo show periodic peaks during hot summer days, confirming a `demand increase during heatwaves and high footfall events`.\n",
    "- Ciudad del Rock exhibits a similar pattern, given it is the closest petrol station near the `Mad Cool festival` with distinct sales peaks aligning with festival dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ba3df-338f-48a9-afaa-a87e4d009f5f",
   "metadata": {},
   "source": [
    "## **E. Predictive Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f31f48-0d66-4c88-b8e1-c2eef330b588",
   "metadata": {},
   "source": [
    "Apply k-means clustering to segment petrol stations based on external factors influencing customer behavior, such as temperature, weekly patterns, festivals, total sales, and average sales. Additionally, create derived features to identify which factors are most important for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac667bb-e88b-4e85-9c51-3e8a1cbbab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset (ensure 'Date' column is in datetime format)\n",
    "df = pd.read_excel('filtered_data_65km_2021_2022.xlsx')\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df[df[\"Year\"] == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4931b-1740-4f2f-b6f6-d52bd47a8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset (ensure 'Date' column is in datetime format)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df[df[\"Year\"] == 2022]\n",
    "# Define Weekend, Hot Day, and Festival Features\n",
    "df[\"Is_Weekend\"] = df[\"Date\"].dt.weekday.isin([5, 6]).astype(int)  # Saturday & Sunday = 1\n",
    "df[\"Hot_Day\"] = (df[\"Max_Temp_Celsius\"] > 35).astype(int)  # Customize hot temp threshold\n",
    "\n",
    "# Manually add festival dates\n",
    "festival_dates = [\"2022-06-25\", \"2022-06-17\", \"2022-06-18\", \"2022-08-06\"]\n",
    "df[\"Is_Festival\"] = df[\"Date\"].isin(pd.to_datetime(festival_dates)).astype(int)\n",
    "\n",
    "# Aggregate Data at Petrol Station Level\n",
    "agg_df = df.groupby(\"Petrol_Station_Code\").agg(\n",
    "    avg_sales=(\"Sold_Units\", \"mean\"),\n",
    "    std_sales=(\"Sold_Units\", \"std\"),\n",
    "    avg_temp=(\"Avg_Temp_Celsius\", \"mean\"),\n",
    "    avg_tmax=(\"Max_Temp_Celsius\", \"mean\"),\n",
    "    total_precip=(\"Total_Precipitation_mm\", \"sum\"),\n",
    "    weekend_sales=(\"Sold_Units\", lambda x: x[df[\"Is_Weekend\"] == 1].mean()),\n",
    "    festival_sales=(\"Sold_Units\", lambda x: x[df[\"Is_Festival\"] == 1].mean()),\n",
    "    hot_day_sales=(\"Sold_Units\", lambda x: x[df[\"Hot_Day\"] == 1].mean()),\n",
    "    weekend_ratio=(\"Is_Weekend\", \"mean\"),\n",
    "    festival_ratio=(\"Is_Festival\", \"mean\"),\n",
    "    hot_day_ratio=(\"Hot_Day\", \"mean\"),\n",
    "    avg_wind_speed=(\"Avg_Wind_Speed_kph\", \"mean\"),\n",
    "    avg_pressure=(\"Avg_Sea_Level_Pressure_hpa\", \"mean\"),\n",
    "    avg_distance=(\"Distance_From_Madrid\", \"mean\")\n",
    ").fillna(0)\n",
    "\n",
    "# Normalize Data for Clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(agg_df)\n",
    "\n",
    "# Determine Optimal K Using Elbow Method\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), wcss, marker=\"o\", linestyle=\"--\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means Clustering with Optimal K\n",
    "optimal_k = 3  # Adjust based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "agg_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Merge Back Location Info\n",
    "agg_df = agg_df.merge(df[[\"Petrol_Station_Code\", \"Province\", \"Town_City\", \"Postcode\", \"Company_Code\"]].drop_duplicates(), \n",
    "                      on=\"Petrol_Station_Code\", how=\"left\")\n",
    "\n",
    "# Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=agg_df, x=\"avg_sales\", y=\"hot_day_sales\", hue=\"Cluster\", palette=\"viridis\")\n",
    "plt.xlabel(\"Average Sales\")\n",
    "plt.ylabel(\"Hot Day Sales\")\n",
    "plt.title(\"Clustering of Petrol Stations\")\n",
    "plt.show()\n",
    "\n",
    "# Print Cluster Analysis (only numeric columns)\n",
    "numeric_cols = agg_df.select_dtypes(include=[\"number\"]).columns\n",
    "print(agg_df.groupby(\"Cluster\")[numeric_cols].mean())\n",
    "\n",
    "# Print Stations in Each Cluster\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nStations in Cluster {cluster}:\")\n",
    "    print(agg_df[agg_df[\"Cluster\"] == cluster][[\"Petrol_Station_Code\", \"Province\", \"Town_City\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61721530-4af4-4f6f-bd26-0a899da51a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means Clustering with k=2\n",
    "optimal_k = 2  # Adjust based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "agg_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "\n",
    "# Visualize Clusters (using avg_sales vs hot_day_sales for simplicity)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=agg_df, x=\"avg_sales\", y=\"hot_day_sales\", hue=\"Cluster\", palette=\"viridis\", s=100, alpha=0.7)\n",
    "plt.xlabel(\"Average Sales\")\n",
    "plt.ylabel(\"Hot Day Sales\")\n",
    "plt.title(f\"Clustering of Petrol Stations (K={optimal_k} Clusters)\")\n",
    "plt.show()\n",
    "\n",
    "# Print Cluster Analysis (only numeric columns)\n",
    "numeric_cols = agg_df.select_dtypes(include=[\"number\"]).columns\n",
    "print(f\"\\nCluster Analysis (K={optimal_k}):\")\n",
    "print(agg_df.groupby(\"Cluster\")[numeric_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c021b-4711-4d5d-8a2c-bd75dbd00e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means Clustering with k=4\n",
    "optimal_k = 4  # Adjust based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "agg_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "\n",
    "# Visualize Clusters (using avg_sales vs hot_day_sales for simplicity)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=agg_df, x=\"avg_sales\", y=\"hot_day_sales\", hue=\"Cluster\", palette=\"viridis\", s=100, alpha=0.7)\n",
    "plt.xlabel(\"Average Sales\")\n",
    "plt.ylabel(\"Hot Day Sales\")\n",
    "plt.title(f\"Clustering of Petrol Stations (K={optimal_k} Clusters)\")\n",
    "plt.show()\n",
    "\n",
    "# Print Cluster Analysis (only numeric columns)\n",
    "numeric_cols = agg_df.select_dtypes(include=[\"number\"]).columns\n",
    "print(f\"\\nCluster Analysis (K={optimal_k}):\")\n",
    "print(agg_df.groupby(\"Cluster\")[numeric_cols].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966ee66-bf8d-4187-a8db-ed45b527eb9b",
   "metadata": {},
   "source": [
    "Final model contains 4 clusters as it shows as each cluster shows the difference in sales patterns and different factors shape the consumer brahviours. The distance from madrid was removed as it was causing some overalping between clusters. \n",
    "Next step is to create 3 models for clusters: 1,2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc464f-eeda-41db-83ab-f256e5ec8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Define Weekend, Hot Day, and Festival Features\n",
    "df[\"Is_Weekend\"] = df[\"Date\"].dt.weekday.isin([5, 6]).astype(int)  # Saturday & Sunday = 1\n",
    "df[\"Hot_Day\"] = (df[\"Max_Temp_Celsius\"] > 35).astype(int)  # Customize hot temp threshold\n",
    "\n",
    "# Manually add festival dates\n",
    "festival_dates = [\"2022-06-25\", \"2022-06-17\", \"2022-06-18\", \"2022-08-06\"]\n",
    "df[\"Is_Festival\"] = df[\"Date\"].isin(pd.to_datetime(festival_dates)).astype(int)\n",
    "\n",
    "# Aggregate Data at Petrol Station Level\n",
    "agg_df = df.groupby(\"Petrol_Station_Code\").agg(\n",
    "    avg_sales=(\"Sold_Units\", \"mean\"),\n",
    "    std_sales=(\"Sold_Units\", \"std\"),\n",
    "    avg_tmax=(\"Max_Temp_Celsius\", \"mean\"),\n",
    "    weekend_sales=(\"Sold_Units\", lambda x: x[df[\"Is_Weekend\"] == 1].mean()),\n",
    "    festival_sales=(\"Sold_Units\", lambda x: x[df[\"Is_Festival\"] == 1].mean()),\n",
    "    hot_day_sales=(\"Sold_Units\", lambda x: x[df[\"Hot_Day\"] == 1].mean()),\n",
    "    weekend_ratio=(\"Is_Weekend\", \"mean\"),\n",
    "    festival_ratio=(\"Is_Festival\", \"mean\"),\n",
    "    hot_day_ratio=(\"Hot_Day\", \"mean\"),\n",
    "    avg_wind_speed=(\"Avg_Wind_Speed_kph\", \"mean\"),\n",
    "    avg_pressure=(\"Avg_Sea_Level_Pressure_hpa\", \"mean\")\n",
    ").fillna(0)\n",
    "\n",
    "# Normalize Data for Clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(agg_df)\n",
    "\n",
    "# Determine Optimal K Using Elbow Method\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Apply K-Means Clustering with Optimal K\n",
    "optimal_k = 4  # Adjust based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "agg_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Merge Back Location Info\n",
    "agg_df = agg_df.merge(df[[\"Petrol_Station_Code\", \"Province\", \"Town_City\", \"Postcode\", \"Company_Code\"]].drop_duplicates(), \n",
    "                      on=\"Petrol_Station_Code\", how=\"left\")\n",
    "\n",
    "# Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=agg_df, x=\"avg_sales\", y=\"hot_day_sales\", hue=\"Cluster\", palette=\"viridis\")\n",
    "plt.xlabel(\"Average Sales\")\n",
    "plt.ylabel(\"Hot Day Sales\")\n",
    "plt.title(\"Clustering of Petrol Stations\")\n",
    "plt.show()\n",
    "\n",
    "# Print Cluster Analysis (only numeric columns)\n",
    "numeric_cols = agg_df.select_dtypes(include=[\"number\"]).columns\n",
    "print(agg_df.groupby(\"Cluster\")[numeric_cols].mean())\n",
    "\n",
    "# Print Stations in Each Cluster\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nStations in Cluster {cluster}:\")\n",
    "    print(agg_df[agg_df[\"Cluster\"] == cluster][[\"Petrol_Station_Code\",\"Postcode\",\"Town_City\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30a9ef-10be-4a60-b537-8aa8270fa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart to present a total sales per cluster.\n",
    "# Define a fixed color mapping for clusters (adjust colors if needed)\n",
    "cluster_colors = {0: \"#440154\", 1: \"#31688E\", 2: \"#35B779\", 3: \"#FDE725\"}  \n",
    "\n",
    "# Aggregate total sales by cluster\n",
    "sales_by_cluster = agg_df.groupby(\"Cluster\")[\"avg_sales\"].sum()\n",
    "\n",
    "# Get clusters in the correct order as they appear in sales_by_cluster\n",
    "clusters = sales_by_cluster.index  \n",
    "\n",
    "# Apply colors based on the cluster numbers\n",
    "colors = [cluster_colors[c] for c in clusters if c in cluster_colors]  \n",
    "\n",
    "# Plot Pie Chart for Sales\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sales_by_cluster, labels=clusters, autopct=\"%1.1f%%\", \n",
    "        colors=colors, startangle=140, wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "# Title and save\n",
    "plt.title(\"Total Sales by Cluster (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44c721-419a-4727-bbeb-0bb6cf162f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart to present a petrol stations distribution.\n",
    "# Define a fixed color mapping for clusters (adjust as needed)\n",
    "cluster_colors = {0: \"#440154\", 1: \"#31688E\", 2: \"#35B779\", 3: \"#FDE725\"}  \n",
    "\n",
    "# Count the number of petrol stations in each cluster\n",
    "cluster_counts = agg_df[\"Cluster\"].value_counts()\n",
    "\n",
    "# Get labels in the correct order (as they appear in value_counts)\n",
    "clusters = cluster_counts.index  \n",
    "\n",
    "# Apply colors based on the cluster numbers\n",
    "colors = [cluster_colors[c] for c in clusters if c in cluster_colors]  \n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(cluster_counts, labels=clusters, autopct=\"%1.1f%%\", \n",
    "        colors=colors, startangle=140, wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "# Title and save\n",
    "plt.title(\"Distribution of Petrol Stations by Cluster (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d85512-4b37-4518-a6b9-b14a0a3bd12e",
   "metadata": {},
   "source": [
    "### Predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2c2b8-6de1-4ae3-a9ab-b96ce3a3332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree \n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import cross_val_score # Often used, not demonstrated here\n",
    "#import imblearn # Not used in this demonstration\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e00d57-208a-4475-bff8-f6eb1068adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "df = pd.read_excel('filtered_data_65km_2021_2022.xlsx')\n",
    "\n",
    "# Define festival dates and convert to datetime\n",
    "festival_dates = [\"2022-06-25\", \"2022-08-06\", \"2022-06-17\", \"2022-06-18\"]\n",
    "festival_dates = pd.to_datetime(festival_dates)\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Add a binary column for festival days\n",
    "df[\"Festival\"] = df[\"Date\"].isin(festival_dates).astype(int)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31daca-d6a6-4e56-bfeb-10cf98a0780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Province','Town_City','Company_Code','Latitude','Longitude',\n",
    "                   'Distance_From_Madrid','Postcode',\n",
    "                   'Petrol_Station_Name'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e6a35-766d-4f79-9872-53b8090d8d73",
   "metadata": {},
   "source": [
    "### Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867e989-21aa-4935-b792-0a3c801f2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to create df contatning 2022 data and for cluster 1\n",
    "df_1 = df[(df['Year'] == 2022) & (df['Petrol_Station_Code'].isin([355, 905, 990, 1768]))]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9902a-e4ad-407d-9866-b4067d587471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding derrived features\n",
    "# Ensure 'Date' column is in datetime format\n",
    "df_1['Date'] = pd.to_datetime(df_1['Date'])\n",
    "\n",
    "# Sort by date to ensure correct rolling calculations\n",
    "df_1 = df_1.sort_values(by='Date')\n",
    "\n",
    "# Add 7-day rolling max temperature\n",
    "df_1['tmax_7d_rolling_max'] = df_1['Max_Temp_Celsius'].rolling(window=7, min_periods=1).max()\n",
    "\n",
    "# Add lag columns (temperature difference from past days)\n",
    "df_1['tmax_lag_3d'] = df_1['Max_Temp_Celsius'] - df_1['Max_Temp_Celsius'].shift(3)  # 3-day lag\n",
    "df_1['tmax_lag_7d'] = df_1['Max_Temp_Celsius'] - df_1['Max_Temp_Celsius'].shift(7)  # 7-day lag\n",
    "\n",
    "# Function to assign seasons based on month\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Autumn\"\n",
    "\n",
    "# Apply function to create 'Season' column\n",
    "df_1['Season'] = df_1['Date'].apply(get_season)\n",
    "\n",
    "# Convert 'Season' to categorical type\n",
    "df_1['Season'] = df_1['Season'].astype('category')\n",
    "\n",
    "# Optionally, encode 'Season' numerically (for ML models)\n",
    "df_1['Season_encoded'] = df_1['Season'].cat.codes\n",
    "\n",
    "# Filter out rows where the 'Season' is 'Winter'\n",
    "df_1 = df_1[df_1['Season'] != 'Winter']\n",
    "\n",
    "# Display sample of updated DataFrame\n",
    "print(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa037f-f4f6-4ba3-a4df-b1f9b0cbbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessery libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"Avg_Temp_Celsius\", \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_1[features], df_1[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd7b08-7acc-41ab-829a-4c4a0d313fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new derrived features\n",
    "# Ensure data is sorted by date before calculating rolling averages\n",
    "df_1 = df_1.sort_values(by=\"Date\")\n",
    "\n",
    "# Add 7-day rolling average of Sold Units\n",
    "df_1[\"sold_units_7d_avg\"] = df_1[\"Sold_Units\"].rolling(window=7, min_periods=1).mean()\n",
    "# Day-to-day temperature difference\n",
    "df_1['temp_diff'] = df_1['Max_Temp_Celsius'] - df_1['Max_Temp_Celsius'].shift(1)  \n",
    "# Extract day of the week (0 = Monday, 6 = Sunday)\n",
    "df_1['Day_of_Week'] = pd.to_datetime(df_1['Date']).dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d477d1-50b4-456c-a7c6-dc6922a5805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly added feature\n",
    "features = [\n",
    "    \"Avg_Temp_Celsius\", \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\"temp_diff\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\",\n",
    "    \"sold_units_7d_avg\",\"Day_of_Week\" \n",
    "]\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_1[features], df_1[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535afd68-274d-44ba-bcc8-2888936cdb14",
   "metadata": {},
   "source": [
    "##### Improving Model Accuracy with Random Forest\n",
    "Initially, a Decision Tree model was used for classification. However, to enhance accuracy and reduce overfitting, I decided to implement a Random Forest model. Random Forest combines multiple decision trees to improve prediction performance and generalization. Below is the implementation of the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ad125-4ac5-4aac-972b-b0e7a8a099b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest to improve accuracy.\n",
    "# Import relevant libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"Max_Temp_Celsius\",\n",
    "     \"Day_of_Week\",\n",
    "    \"sold_units_7d_avg\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_1[features], df_1[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot the individual decision trees in the forest (just the first tree for simplicity)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=features, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db5ad2-8fb4-4799-aae2-daae51e54ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model A\n",
    "# Define business-friendly feature names\n",
    "features_rename = {\n",
    "    \"Max_Temp_Celsius\": \"Maximum Temperature\",\n",
    "    \"Day_of_Week\": \"Day of the Week\",\n",
    "    \"sold_units_7d_avg\": \"7-Day Average Sales\"\n",
    "}\n",
    "\n",
    "# Original feature names for DataFrame column access\n",
    "original_features = list(features_rename.keys())\n",
    "\n",
    "# Update the features list to use the new names for display\n",
    "features = [features_rename[f] for f in original_features]\n",
    "\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets (use original feature names)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_1[original_features], df_1[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature importance with business-friendly names\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot the individual decision trees in the forest (just the first tree for simplicity)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=features, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance values with business-friendly names\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297d552-e2aa-4eb9-b4c4-66c4e0a76c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importance values with business-friendly names\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f8e11-d401-4929-b804-b2cd945ae04f",
   "metadata": {},
   "source": [
    "### Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef3864-dac2-45f7-a170-9fa1919460af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of petrol station codes for Cluster 2\n",
    "cluster_2_codes = [\n",
    "    1, 2, 5, 312, 437, 465, 527, 805, 808, 859, 869, 896, 932, 934, 935,\n",
    "    996, 1104, 1272, 1288, 1289, 1296, 1297, 1350, 1401, 1402, 1592, 1667, \n",
    "    1669, 1671, 1688, 1781, 1837, 1845, 1866, 1870, 1871, 1872, 1878, 1879, \n",
    "    1895, 1903, 1933, 1970\n",
    "]\n",
    "\n",
    "# Filter data to create df containing 2022 data and for Cluster 2\n",
    "df_2 = df[(df['Year'] == 2022) & (df['Petrol_Station_Code'].isin(cluster_2_codes))]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee2dc0-a30b-4f55-a148-ac54ac8d7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is in datetime format\n",
    "df_2['Date'] = pd.to_datetime(df_2['Date'])\n",
    "\n",
    "# Sort by date to ensure correct rolling calculations\n",
    "df_2 = df_2.sort_values(by='Date')\n",
    "\n",
    "# Add 7-day rolling max temperature\n",
    "df_2['tmax_7d_rolling_max'] = df_2['Max_Temp_Celsius'].rolling(window=7, min_periods=1).max()\n",
    "\n",
    "# Add lag columns (temperature difference from past days)\n",
    "df_2['tmax_lag_3d'] = df_2['Max_Temp_Celsius'] - df_2['Max_Temp_Celsius'].shift(3)  # 3-day lag\n",
    "df_2['tmax_lag_7d'] = df_2['Max_Temp_Celsius'] - df_2['Max_Temp_Celsius'].shift(7)  # 7-day lag\n",
    "\n",
    "# Function to assign seasons based on month\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Autumn\"\n",
    "\n",
    "# Apply function to create 'Season' column\n",
    "df_2['Season'] = df_2['Date'].apply(get_season)\n",
    "\n",
    "# Convert 'Season' to categorical type\n",
    "df_2['Season'] = df_2['Season'].astype('category')\n",
    "\n",
    "# Optionally, encode 'Season' numerically (for ML models)\n",
    "df_2['Season_encoded'] = df_2['Season'].cat.codes\n",
    "\n",
    "# Filter out rows where the 'Season' is 'Winter'\n",
    "df_2 = df_2[df_2['Season'] != 'Winter']\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_2.head()\n",
    "# Display sample of updated DataFrame\n",
    "print(df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdd36f-70a1-47b9-8aba-795a49de716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = [\n",
    "    \"Avg_Temp_Celsius\", \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2[features], df_2[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4053c4-186d-4031-9097-7258f2cb1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by date before calculating rolling averages\n",
    "df_2 = df_2.sort_values(by=\"Date\")\n",
    "\n",
    "# Add 7-day rolling average of Sold Units\n",
    "df_2[\"sold_units_7d_avg\"] = df_2[\"Sold_Units\"].rolling(window=7, min_periods=1).mean()\n",
    "# Day-to-day temperature difference\n",
    "df_2['temp_diff'] = df_2['Max_Temp_Celsius'] - df_2['Max_Temp_Celsius'].shift(1)  \n",
    "# Extract day of the week (0 = Monday, 6 = Sunday)\n",
    "df_2['Day_of_Week'] = pd.to_datetime(df_2['Date']).dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4bf8f-1541-4f68-bdb8-1722cb8f1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly added feature\n",
    "features = [\n",
    "    \"Avg_Temp_Celsius\", \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\"temp_diff\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\"\n",
    "]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2[features], df_2[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce307e-d408-41f8-94e3-9e70c15bf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted features\n",
    "features = [\n",
    "    \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\"temp_diff\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\",\n",
    "    \"Day_of_Week\"]\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2[features], df_2[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07692a77-3133-4091-9a46-d3de146a26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Total_Precipitation_mm\",\n",
    "    \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\",\n",
    "    \"Day_of_Week\"]\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2[features], df_2[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=7, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5dc976-df1d-4346-bbe7-0b99498fda37",
   "metadata": {},
   "source": [
    "##### Improving Model Accuracy with Random Forest\n",
    "Initially, a Decision Tree model was used for classification. However, to enhance accuracy and reduce overfitting, I decided to implement a Random Forest model. Random Forest combines multiple decision trees to improve prediction performance and generalization. Below is the implementation of the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb626f7-6f62-4d3e-a9c4-6234c850d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model B\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\",\n",
    "    \"Day_of_Week\",\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2[features], df_2[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot the individual decision trees in the forest (just the first tree for simplicity)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=features, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5e110-d802-45ae-aff1-dbbff1ca9483",
   "metadata": {},
   "source": [
    "### Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f250c3-fa44-46b9-b941-432cf9a2af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of petrol station codes for Cluster 3\n",
    "cluster_3_codes = [\n",
    "    4, 287, 436, 459, 497, 701, 738, 753, 754, 835, 858, 922, 948, 952, 983, \n",
    "    986, 1034, 1056, 1064, 1132, 1165, 1274, 1299, 1304, 1400, 1453, 1454, \n",
    "    1481, 1521, 1525, 1636, 1641, 1658, 1668, 1731, 1824, 1888, 1892, 1894, \n",
    "    1902, 1926, 1946, 1947, 1972\n",
    "]\n",
    "\n",
    "# Filter data to create df containing 2022 data for Cluster 3\n",
    "df_3 = df[(df['Year'] == 2022) & (df['Petrol_Station_Code'].isin(cluster_3_codes))]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85ed07-d826-4618-ae8d-d89b2ba67a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is in datetime format\n",
    "df_3['Date'] = pd.to_datetime(df_3['Date'])\n",
    "\n",
    "# Sort by date to ensure correct rolling calculations\n",
    "df_3 = df_3.sort_values(by='Date')\n",
    "\n",
    "# Add 7-day rolling max temperature\n",
    "df_3['tmax_7d_rolling_max'] = df_3['Max_Temp_Celsius'].rolling(window=7, min_periods=1).max()\n",
    "\n",
    "# Add lag columns (temperature difference from past days)\n",
    "df_3['tmax_lag_3d'] = df_3['Max_Temp_Celsius'] - df_3['Max_Temp_Celsius'].shift(3)  # 3-day lag\n",
    "df_3['tmax_lag_7d'] = df_3['Max_Temp_Celsius'] - df_3['Max_Temp_Celsius'].shift(7)  # 7-day lag\n",
    "\n",
    "# Function to assign seasons based on month\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Autumn\"\n",
    "\n",
    "# Apply function to create 'Season' column\n",
    "df_3['Season'] = df_3['Date'].apply(get_season)\n",
    "\n",
    "# Convert 'Season' to categorical type\n",
    "df_3['Season'] = df_3['Season'].astype('category')\n",
    "\n",
    "# Optionally, encode 'Season' numerically (for ML models)\n",
    "df_3['Season_encoded'] = df_3['Season'].cat.codes\n",
    "\n",
    "# Filter out rows where the 'Season' is 'Winter'\n",
    "df_3 = df_3[df_3['Season'] != 'Winter']\n",
    "\n",
    "# Display sample of updated DataFrame\n",
    "print(df_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987c841-0599-4937-a393-d482ffabf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = [\n",
    "    \"Avg_Temp_Celsius\", \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"tmax_lag_3d\", \"tmax_lag_7d\", \"Season_encoded\", \"Festival\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_3[features], df_3[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53010301-68c3-47dc-8d62-158d6af69e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted by date before calculating rolling averages\n",
    "df_3 = df_3.sort_values(by=\"Date\")\n",
    "\n",
    "# Add 7-day rolling average of Sold Units\n",
    "df_3[\"sold_units_7d_avg\"] = df_3[\"Sold_Units\"].rolling(window=7, min_periods=1).mean()\n",
    "df_3['temp_diff'] = df_3['Max_Temp_Celsius'] - df_3['Max_Temp_Celsius'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d65ee2-be23-41d1-93fe-09e9c923e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features = [\n",
    "   \"Max_Temp_Celsius\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"Festival\"\n",
    "]\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_3[features], df_3[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726aba5-9544-4e6f-ba52-5be716ec5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of the week (0 = Monday, 6 = Sunday)\n",
    "df_3['Day_of_Week'] = pd.to_datetime(df_3['Date']).dt.dayofweek\n",
    "df_3[\"Weekend_Festival\"] = ((df_3[\"Day_of_Week\"] >= 5) & (df_3[\"Festival\"] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a138b21-5a92-47fc-a80e-14e38c318386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly added feature\n",
    "features = [\n",
    "    \"Max_Temp_Celsius\", \"Total_Precipitation_mm\",\"temp_diff\",\n",
    "    \"Avg_Wind_Direction_deg\", \"Avg_Wind_Speed_kph\", \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"tmax_7d_rolling_max\", \"Festival\",\"Weekend_Festival\"\n",
    "    ]\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_3[features], df_3[\"Sold_Units\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "model = DecisionTreeRegressor(max_depth=12, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Decision Tree Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(25, 12))  # Increase figure size\n",
    "tree.plot_tree(\n",
    "    model, \n",
    "    feature_names=features, \n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    fontsize=10  # Increase font size for readability\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7d1b3-92b4-4222-918e-c5ee5f6a055d",
   "metadata": {},
   "source": [
    "##### Improving Model Accuracy with Random Forest\n",
    "Initially, a Decision Tree model was used for classification. However, to enhance accuracy and reduce overfitting, I decided to implement a Random Forest model. Random Forest combines multiple decision trees to improve prediction performance and generalization. Below is the implementation of the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d4ada-cb5d-42e0-a119-3f980efe95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "     \"tmax_7d_rolling_max\",\n",
    "    \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"Weekend_Festival\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_3[features], df_3[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Display feature importance as a list\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot the individual decision trees in the forest (just the first tree for simplicity)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=features, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535591af-6ebe-45e4-b486-e179e1a7014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [3, 5, 10],  # Depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples to split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Optimized MAE: {mae:.2f}\")\n",
    "print(f\"Optimized RMSE: {rmse:.2f}\")\n",
    "print(f\"Optimized R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bfb32-d0f9-4bdd-b53d-2a5e6bc1c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"tmax_7d_rolling_max\",\n",
    "    \"Avg_Sea_Level_Pressure_hpa\",\n",
    "    \"Weekend_Festival\"\n",
    "]\n",
    "target = \"Sold_Units\"\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_3[features], df_3[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model with best parameters\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, min_samples_split=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Display feature importance as a list\n",
    "feature_importance = rf_model.feature_importances_\n",
    "importance_list = sorted(zip(features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for feature, importance in importance_list:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(np.array(features)[sorted_idx], feature_importance[sorted_idx], color=\"royalblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot the individual decision trees in the forest (just the first tree for simplicity)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=features, filled=True, rounded=True, fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9cc19-4099-498a-931e-63a53230df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for feature importances in each model\n",
    "model_A_features = ['Maximum Temperature', 'Day of the Week', '7-Day Average Sales']\n",
    "model_A_importance = [0.0901, 0.2718, 0.6381]\n",
    "\n",
    "model_B_features = ['Average Sea Level Pressure (hPa)', '7-Day Rolling Max Temperature', 'Day of the Week']\n",
    "model_B_importance = [0.0423, 0.5751, 0.3825]\n",
    "\n",
    "model_C_features = ['7-Day Rolling Max Temperature', 'Average Sea Level Pressure (hPa)', 'Weekend or Festival Indicator']\n",
    "model_C_importance = [0.6348, 0.1834, 0.1817]\n",
    "\n",
    "# Function to sort features by importance (descending order)\n",
    "def sort_features(features, importance):\n",
    "    sorted_indices = np.argsort(importance)[::-1]  # Get sorted indices in descending order\n",
    "    sorted_features = [features[i] for i in sorted_indices]\n",
    "    sorted_importance = [importance[i] for i in sorted_indices]\n",
    "    return sorted_features, sorted_importance\n",
    "\n",
    "# Sort features for each model\n",
    "model_A_features, model_A_importance = sort_features(model_A_features, model_A_importance)\n",
    "model_B_features, model_B_importance = sort_features(model_B_features, model_B_importance)\n",
    "model_C_features, model_C_importance = sort_features(model_C_features, model_C_importance)\n",
    "\n",
    "# Create subplots to plot all models on one figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "# Define font sizes\n",
    "title_fontsize = 16\n",
    "label_fontsize = 12\n",
    "tick_fontsize = 12\n",
    "\n",
    "# Plot for Model A\n",
    "axes[0].barh(model_A_features, model_A_importance, color='skyblue')\n",
    "axes[0].set_title(\"Model A\", fontsize=title_fontsize)\n",
    "axes[0].set_xlabel('Importance', fontsize=label_fontsize)\n",
    "axes[0].tick_params(axis='y', labelsize=tick_fontsize)  # Increase feature label font size\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Plot for Model B\n",
    "axes[1].barh(model_B_features, model_B_importance, color='salmon')\n",
    "axes[1].set_title(\"Model B\", fontsize=title_fontsize)\n",
    "axes[1].set_xlabel('Importance', fontsize=label_fontsize)\n",
    "axes[1].tick_params(axis='y', labelsize=tick_fontsize)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Plot for Model C\n",
    "axes[2].barh(model_C_features, model_C_importance, color='lightgreen')\n",
    "axes[2].set_title(\"Model C\", fontsize=title_fontsize)\n",
    "axes[2].set_xlabel('Importance', fontsize=label_fontsize)\n",
    "axes[2].tick_params(axis='y', labelsize=tick_fontsize)\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('models5.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c36b24-0811-46b2-a4ee-879cd86746cd",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this analysis, we evaluated the performance of three predictive models across different customer clusters. The models exhibited varying levels of accuracy and different key drivers of sales, highlighting the distinct characteristics of each cluster.\n",
    "##### Model A (Cluster 1)\n",
    "Performance: Model A achieved the highest R² score (0.67), indicating a strong predictive ability. However, it also had the highest MAE (8.76) and RMSE (16.42), suggesting greater absolute errors in predictions.\n",
    "Key Features: The 7-day average sales (0.6381) had the strongest impact, followed by day of the week (0.2718), while maximum temperature (0.0901) had a minor influence.\n",
    "Insight: Cluster 1’s sales are primarily driven by past sales trends, with some influence from the day of the week, implying a relatively stable and trend-driven purchasing pattern.\n",
    "##### Model B (Cluster 2)\n",
    "Performance: Model B had a moderate MAE (5.87) and RMSE (10.11), but its R² score (0.41) indicates weaker predictive power compared to Model A.\n",
    "Key Features: The most influential feature was day of the week (0.5636), while weather-related variables such as tmax_7d_rolling_max (0.0665) and average sea level pressure (0.0052) had minimal impact.\n",
    "Insight: Cluster 2’s sales patterns are heavily dependent on the day of the week, suggesting customers in this segment follow specific weekday-related purchasing behaviors rather than being influenced by past sales trends.\n",
    "##### Model C (Cluster 3)\n",
    "Performance: Model C had the lowest MAE (3.29) and RMSE (5.68) but also the lowest R² score (0.21), indicating that while individual predictions were more precise, the model struggles to explain overall sales variance.\n",
    "Key Features: The dominant factor was tmax_7d_rolling_max (0.5532), followed by average sea level pressure (0.3199) and weekend festival occurrences (0.1269).\n",
    "Insight: Cluster 3 appears to be significantly influenced by weather conditions, particularly maximum temperature and sea-level pressure, as well as special events such as festivals. This suggests that sales in this segment may be seasonal or event-driven rather than following consistent trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d78c1b-db2e-4600-a531-a4afc46da877",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9846d-62b0-4b21-b8c5-2530fe8569c5",
   "metadata": {},
   "source": [
    "### **Model Performance Summary**  \n",
    "\n",
    "- **Model A (Cluster 1)**: Achieved the highest **R² score (0.67)**, indicating strong predictive power, but had the highest **MAE (8.76)** and **RMSE (16.42)**. Sales are primarily driven by **past trends (7-day average sales)** and moderately influenced by **day of the week**.  \n",
    "- **Model B (Cluster 2)**: Moderate predictive accuracy (**R² = 0.41**) with sales primarily influenced by **day of the week**. Temperature has a minor impact.  \n",
    "- **Model C (Cluster 3)**: Lowest **R² score (0.21)** but lowest **MAE (3.29)**, indicating precise individual predictions but weak overall explanatory power. Sales are highly influenced by **temperature, pressure, and festivals**.  \n",
    "\n",
    "### **Cluster Characteristics & Sales Patterns**  \n",
    "\n",
    "| Cluster  | Distribution | Contribution to Total Sales | Key Sales Drivers | Location Type |\n",
    "|----------|-------------|---------------------------|------------------|--------------|\n",
    "| **0**  | Very Low  | Minimal | No clear pattern | Rural & airport |\n",
    "| **1**  | 4.3%  | 11%  | Festivals, weekends, high temperature | Near motorways, tourist spots |\n",
    "| **2**  | 45.7% | 58.2% | Weekends, high temperature | Urban & suburban |\n",
    "| **3**  | 46.8% | 29.9% | Temperature, festivals | Urban & suburban |\n",
    "\n",
    "### **Recommendations**  \n",
    "\n",
    "1. **Anticipating Demand: A Smart Stocking Strategy**  \n",
    "   - **Clusters**: 1 & 2  \n",
    "   - **Action**: Implement predictive models based on **weather spikes, festival sales trends, and real-time temperature monitoring**.  \n",
    "   - **Logic**: Demand surges **above 30°C**, even more significantly **above 35°C**. Stock adjustments can prevent lost revenue.  \n",
    "\n",
    "2. **Optimizing Supply Chain for High-Demand Locations**  \n",
    "   - **Clusters**: 0, 1 & 2  \n",
    "   - **Action**:  \n",
    "     - Prioritize deliveries to **Clusters 1 & 2** before weekends & festivals.  \n",
    "     - Reduce stock in **Cluster 0** to minimize waste.  \n",
    "   - **Logic**: Clusters 1 and 2 drive **high sales volume**, requiring proactive restocking.  \n",
    "\n",
    "3. **Heatwave Readiness Plan**  \n",
    "   - **Clusters**: 1, 2, and 3  \n",
    "   - **Action**:  \n",
    "     - Deploy **emergency stock reserves** when temperatures exceed **35°C**.  \n",
    "     - Use regional storage for **rapid distribution**.  \n",
    "   - **Logic**: Ensures **continuous availability** and prevents logistical delays.  \n",
    "\n",
    "4. **Improving Pricing & Promotional Strategies**  \n",
    "   - **Clusters**: 1 & 3  \n",
    "   - **Action**:  \n",
    "     - Introduce a **heatwave pricing model**.  \n",
    "     - Offer **bulk-buy incentives before peak demand**.  \n",
    "     - Launch **festival & weekend-specific promotions**.  \n",
    "   - **Logic**: Dynamic pricing optimizes revenue and customer satisfaction.  \n",
    "\n",
    "5. **Implementing Real-Time Analytics for Demand Monitoring**  \n",
    "   - **Clusters**: All  \n",
    "   - **Action**:  \n",
    "     - Deploy a **real-time dashboard** with Google Analytics integration.  \n",
    "     - Automate stock-level updates using predictive models.  \n",
    "   - **Logic**: Live sales & climate data enhance stocking decisions and **logistics efficiency**.  \n",
    "\n",
    "### **Business Impact & Next Steps**  \n",
    "\n",
    "#### **Key Benefits**  \n",
    "✅ **Revenue Growth**: Prevent stockouts, capture peak demand.  \n",
    "✅ **Operational Efficiency**: Reduce overstocking & waste.  \n",
    "✅ **Customer Satisfaction**: Ensure ice and essential items are available when needed.  \n",
    "\n",
    "#### **Next Steps**  \n",
    "🔹 **Pilot predictive models** at high-demand locations.  \n",
    "🔹 **Test emergency stocking protocols** for heatwave scenarios.  \n",
    "🔹 **Develop a real-time analytics dashboard** for continuous monitoring.  \n",
    "\n",
    "By leveraging predictive analytics, real-time monitoring, and weather-based forecasting, this strategy aims to **enhance operational efficiency, boost sales, and improve customer satisfaction.**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
